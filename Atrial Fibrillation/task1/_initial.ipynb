{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will apply an CNN to extract features and implement a classification task.\n",
    "# Firstly, we should build the model by PyTorch. We provide a baseline model here.\n",
    "# You can use your own model for better performance\n",
    "class Doubleconv_33(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_33, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_35(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_35, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_37(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_37, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Tripleconv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Tripleconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc(input)\n",
    "\n",
    "\n",
    "class Mscnn(nn.Module):\n",
    "    # TODO: Build a better model\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Mscnn, self).__init__()\n",
    "        self.conv11 = Doubleconv_33(ch_in, 64)\n",
    "        self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv12 = Doubleconv_33(64, 128)\n",
    "        self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv13 = Tripleconv(128, 256)\n",
    "        self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv14 = Tripleconv(256, 512)\n",
    "        self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv15 = Tripleconv(512, 512)\n",
    "        self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.out = MLP(512*27, ch_out)\n",
    "\n",
    "    # def __init__(self, ch_in, ch_out):\n",
    "    #     super(Mscnn, self).__init__()\n",
    "    #     self.conv11 = Doubleconv_35(ch_in, 64)\n",
    "    #     self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv12 = Doubleconv_35(64, 128)\n",
    "    #     self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv13 = Tripleconv(128, 256)\n",
    "    #     self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv14 = Tripleconv(256, 512)\n",
    "    #     self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv15 = Tripleconv(512, 512)\n",
    "    #     self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "    #     self.out = MLP(512*27, ch_out)\n",
    "\n",
    "    # def __init__(self, ch_in, ch_out):\n",
    "    #     super(Mscnn, self).__init__()\n",
    "    #     self.conv11 = Doubleconv_37(ch_in, 64)\n",
    "    #     self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv12 = Doubleconv_37(64, 128)\n",
    "    #     self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv13 = Tripleconv(128, 256)\n",
    "    #     self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv14 = Tripleconv(256, 512)\n",
    "    #     self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv15 = Tripleconv(512, 512)\n",
    "    #     self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "    #     self.out = MLP(512*27, ch_out) \n",
    "\n",
    "    def forward(self, x):\n",
    "        c11 = self.conv11(x)\n",
    "        p11 = self.pool11(c11)\n",
    "        c12 = self.conv12(p11)\n",
    "        p12 = self.pool12(c12)\n",
    "        c13 = self.conv13(p12)\n",
    "        p13 = self.pool13(c13)\n",
    "        c14 = self.conv14(p13)\n",
    "        p14 = self.pool14(c14)\n",
    "        c15 = self.conv15(p14)\n",
    "        p15 = self.pool15(c15)\n",
    "        merge = p15.view(p15.size()[0], -1) \n",
    "        output = self.out(merge)\n",
    "        output = F.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to construct the data loader for training. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.io as io\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Random clipping has been implemented, \n",
    "# and you need to add noise and random scaling. \n",
    "# Generally, the scaling should be done before the crop.\n",
    "# In general, do not add scaling and noise enhancement options during testing\n",
    "\n",
    "class ECG_dataset(Dataset):\n",
    "\n",
    "    def __init__(self,base_file,cv=0, is_train=True):\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.file_list=[]\n",
    "        self.base_file=base_file\n",
    "        \n",
    "        for i in range(5):\n",
    "            data=pd.read_csv(base_file+'/cv/cv'+str(i)+'.csv')\n",
    "            self.file_list.append(data.to_numpy())\n",
    "        self.file=None\n",
    "        if is_train:\n",
    "            del self.file_list[cv]\n",
    "            self.file=self.file_list[0]\n",
    "            for i in range(1,4):\n",
    "                self.file=np.append(self.file,self.file_list[i],axis=0)\n",
    "        else:\n",
    "            self.file=self.file_list[cv]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file.shape[0]\n",
    "    \n",
    "\n",
    "    def load_data(self,file_name,label):\n",
    "        #读取数据\n",
    "        mat_file = self.base_file+'/training2017/'+file_name+'.mat'\n",
    "        data = io.loadmat(mat_file)['val']\n",
    "        if label=='N':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='O':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='A':\n",
    "            one_hot=torch.tensor([1])\n",
    "        elif label=='~':\n",
    "            one_hot=torch.tensor([0])\n",
    "            \n",
    "        return data,one_hot\n",
    "\n",
    "\n",
    "    \n",
    "    def crop_padding(self,data,time):\n",
    "        #随机crop\n",
    "        if data.shape[0]<=time:\n",
    "            data=np.pad(data, (0,time-data.shape[0]), 'constant')\n",
    "        elif data.shape[0]>time:\n",
    "            end_index=data.shape[0]-time\n",
    "            start=np.random.randint(0, end_index)\n",
    "            data=data[start:start+time]\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def data_process(self,data):\n",
    "        # 学习论文以及数据集选择合适和采样率\n",
    "        # 并完成随机gaussian 噪声和随机时间尺度放缩\n",
    "        data=data[::3]\n",
    "        data=data-data.mean()\n",
    "        data=data/data.std()\n",
    "        data=self.crop_padding(data,2400)\n",
    "        data=torch.tensor(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name=self.file[idx][1]\n",
    "        label=self.file[idx][2]\n",
    "        data,one_hot=self.load_data(file_name,label)\n",
    "        data=self.data_process(data[0]).unsqueeze(0).float()\n",
    "        one_hot=one_hot.unsqueeze(0).float()\n",
    "\n",
    "        return data, one_hot,file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "def save_loss(fold, value):\n",
    "    path = 'loss' + str(fold) + '.txt'\n",
    "    file = open(path, mode='a+')\n",
    "    file.write(str(value)+'\\n')  \n",
    "    \n",
    "# We will use GPU if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "\n",
    "# Build pre-processing transformation \n",
    "# Note this pre-processing is in PyTorch\n",
    "x_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "])\n",
    "y_transforms = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# TODO: fine tune hyper-parameters\n",
    "batch_size = 128\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion2=torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_ecg_dataset = ECG_dataset('./', is_train=True)\n",
    "train_dataloader = DataLoader(train_ecg_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_ecg_dataset = ECG_dataset('./', is_train=False)\n",
    "test_dataloaders = DataLoader(test_ecg_dataset, batch_size=1)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss:0.23732192: 100%|██████████| 107/107 [01:21<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9154206977425975\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3323700892725342\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss:0.20546038: 100%|██████████| 107/107 [01:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9161536206391088\n",
      "Accuracy: 0.9190615835777126\n",
      "Loss: 0.33602200549121813\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss:0.19318464: 100%|██████████| 107/107 [01:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9171797126942246\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.5114541421726153\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss:0.17886123: 100%|██████████| 107/107 [01:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9205511580181764\n",
      "Accuracy: 0.9114369501466275\n",
      "Loss: 0.400305454193692\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss:0.17040004: 100%|██████████| 107/107 [1:07:15<00:00, 37.72s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9265611257695691\n",
      "Accuracy: 0.9002932551319648\n",
      "Loss: 0.2283869424195758\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss:0.15566676:  15%|█▍        | 16/107 [00:12<01:08,  1.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\U\\Desktop\\ecg1\\train_model.ipynb 单元格 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m process \u001b[39m=\u001b[39m tqdm(train_dataloader)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y,name \u001b[39min\u001b[39;00m process:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     inputs \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\U\\Desktop\\ecg1\\train_model.ipynb 单元格 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m file_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile[idx][\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m label\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile[idx][\u001b[39m2\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m data,one_hot\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data(file_name,label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_process(data[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m one_hot\u001b[39m=\u001b[39mone_hot\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;32mc:\\Users\\U\\Desktop\\ecg1\\train_model.ipynb 单元格 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(\u001b[39mself\u001b[39m,file_name,label):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#读取数据\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     mat_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_file\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/training2017/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfile_name\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     data \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mloadmat(mat_file)[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m label\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/U/Desktop/ecg1/train_model.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         one_hot\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[0;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\U\\.conda\\envs\\proj1\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m file_like, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(file_like, \u001b[39mstr\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, f1_score\n",
    "\n",
    "def validation(model,criterion,test_dataloaders,device):\n",
    "    # TODO: add more metrics for evaluation?\n",
    "    # Evaluate \n",
    "    model.eval()\n",
    "    predict = np.array([])\n",
    "    target = np.array([])\n",
    "    loss=0\n",
    "    step=0\n",
    "    with torch.no_grad():\n",
    "        for x, mask,name in test_dataloaders:\n",
    "            step += 1\n",
    "            mask=mask.to(device)\n",
    "            y = model(x.to(device))\n",
    "            loss +=criterion(y, mask.squeeze(2)).item()\n",
    "            y[y >= 0.5] = 1\n",
    "            y[y < 0.5] = 0\n",
    "            predict=np.append(predict,torch.squeeze(y).cpu().numpy())\n",
    "            target=np.append(target,torch.squeeze(mask).cpu().numpy())\n",
    "    acc = accuracy_score(target, predict)\n",
    "    math_c = matthews_corrcoef(target, predict)\n",
    "    roc_auc_s = roc_auc_score(target, predict)\n",
    "    f1_sco = f1_score(target, predict)\n",
    "    print('Accuracy: {}'.format(acc))\n",
    "    print('Loss:', loss/step)\n",
    "    model.train()\n",
    "\n",
    "    return loss/step, acc, math_c, roc_auc_s, f1_sco\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "matthews_c = []\n",
    "ras = []\n",
    "f1_s = []\n",
    "\n",
    "# Start training !\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "        predict = np.array([])\n",
    "        target = np.array([])\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        dt_size = len(train_dataloader.dataset)\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        process = tqdm(train_dataloader)\n",
    "        for x, y,name in process:\n",
    "            step += 1\n",
    "            inputs = x.to(device)\n",
    "            labels = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion2(outputs, labels.squeeze(2))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            process.set_description(\n",
    "                \"epoch: %d, train_loss:%0.8f\" % (epoch, epoch_loss / step)\n",
    "            )\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            predict=np.append(predict,torch.squeeze(outputs).detach().cpu().numpy())\n",
    "            target=np.append(target,torch.squeeze(labels).detach().cpu().numpy())\n",
    "        epoch_loss /= step\n",
    "        train_loss.append(epoch_loss)\n",
    "        acc = accuracy_score(target, predict)\n",
    "        train_acc.append(acc)\n",
    "        print('tran_Accuracy: {}'.format(acc))\n",
    "        save_loss(10, epoch_loss)\n",
    "        v_loss, v_acc, math_c, roc_auc_s, f1_src = validation(model, criterion2, test_dataloaders, device)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        matthews_c.append(math_c)\n",
    "        f1_s.append(f1_src)\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'weights10_%d.pth' % (epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910850439882698\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_curve(y):\n",
    "    x = np.arange(1, len(y)+1, 1)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "\n",
    "\n",
    "plot_curve(train_loss)\n",
    "plot_curve(train_acc)\n",
    "plot_curve(val_loss)\n",
    "plot_curve(val_acc)\n",
    "plot_curve(matthews_c)\n",
    "plot_curve(ras)\n",
    "plot_curve(f1_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79495757624adbb2ec94ee769202e9d295b98634352b3931ea30316c5ea35353"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('pujin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
