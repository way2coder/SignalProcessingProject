{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will apply an CNN to extract features and implement a classification task.\n",
    "# Firstly, we should build the model by PyTorch. We provide a baseline model here.\n",
    "# You can use your own model for better performance\n",
    "class Doubleconv_33(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_33, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_35(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_35, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_37(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_37, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Tripleconv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Tripleconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc(input)\n",
    "\n",
    "\n",
    "class Mscnn(nn.Module):\n",
    "    # TODO: Build a better model\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Mscnn, self).__init__()\n",
    "        self.conv11 = Doubleconv_33(ch_in, 64)\n",
    "        self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv12 = Doubleconv_33(64, 128)\n",
    "        self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv13 = Tripleconv(128, 256)\n",
    "        self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv14 = Tripleconv(256, 512)\n",
    "        self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv15 = Tripleconv(512, 512)\n",
    "        self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.conv21 = Doubleconv_37(ch_in, 64)\n",
    "        self.pool21 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv22 = Doubleconv_37(64, 128)\n",
    "        self.pool22 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv23 = Tripleconv(128, 256)\n",
    "        self.pool23 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv24 = Tripleconv(256, 512)\n",
    "        self.pool24 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv25 = Tripleconv(512, 512)\n",
    "        self.pool25 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.out = MLP(512*27*2, ch_out)\n",
    "\n",
    "    # def __init__(self, ch_in, ch_out):\n",
    "    #     super(Mscnn, self).__init__()\n",
    "    #     self.conv11 = Doubleconv_35(ch_in, 64)\n",
    "    #     self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv12 = Doubleconv_35(64, 128)\n",
    "    #     self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv13 = Tripleconv(128, 256)\n",
    "    #     self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv14 = Tripleconv(256, 512)\n",
    "    #     self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv15 = Tripleconv(512, 512)\n",
    "    #     self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "    #     self.out = MLP(512*27, ch_out)\n",
    "\n",
    "    # def __init__(self, ch_in, ch_out):\n",
    "    #     super(Mscnn, self).__init__()\n",
    "    #     self.conv11 = Doubleconv_37(ch_in, 64)\n",
    "    #     self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv12 = Doubleconv_37(64, 128)\n",
    "    #     self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "    #     self.conv13 = Tripleconv(128, 256)\n",
    "    #     self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv14 = Tripleconv(256, 512)\n",
    "    #     self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "    #     self.conv15 = Tripleconv(512, 512)\n",
    "    #     self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "    #     self.out = MLP(512*27, ch_out) \n",
    "\n",
    "    def forward(self, x):\n",
    "        c11 = self.conv11(x)\n",
    "        p11 = self.pool11(c11)\n",
    "        c12 = self.conv12(p11)\n",
    "        p12 = self.pool12(c12)\n",
    "        c13 = self.conv13(p12)\n",
    "        p13 = self.pool13(c13)\n",
    "        c14 = self.conv14(p13)\n",
    "        p14 = self.pool14(c14)\n",
    "        c15 = self.conv15(p14)\n",
    "        p15 = self.pool15(c15)\n",
    "\n",
    "        c21 = self.conv21(x)\n",
    "        p21 = self.pool21(c21)\n",
    "        c22 = self.conv22(p21)\n",
    "        p22 = self.pool22(c22)\n",
    "        c23 = self.conv23(p22)\n",
    "        p23 = self.pool23(c23)\n",
    "        c24 = self.conv24(p23)\n",
    "        p24 = self.pool24(c24)\n",
    "        c25 = self.conv25(p24)\n",
    "        p25 = self.pool25(c25)\n",
    "\n",
    "        merge = torch.cat((p15, p25), dim=1)\n",
    "        merge = merge.view(merge.size()[0], -1)\n",
    "        output = self.out(merge)\n",
    "        output = F.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to construct the data loader for training. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.io as io\n",
    "from scipy.interpolate import interp1d\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Random clipping has been implemented, \n",
    "# and you need to add noise and random scaling. \n",
    "# Generally, the scaling should be done before the crop.\n",
    "# In general, do not add scaling and noise enhancement options during testing\n",
    "\n",
    "class ECG_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, base_file, cv=0, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self.file_list=[]\n",
    "        self.base_file=base_file\n",
    "        \n",
    "        for i in range(5):\n",
    "            data=pd.read_csv(base_file+'/cv/cv'+str(i)+'.csv')\n",
    "            self.file_list.append(data.to_numpy())\n",
    "        self.file=None\n",
    "        if is_train:\n",
    "            del self.file_list[cv]\n",
    "            self.file=self.file_list[0]\n",
    "            for i in range(1,4):\n",
    "                self.file=np.append(self.file,self.file_list[i],axis=0)\n",
    "        else:\n",
    "            self.file=self.file_list[cv]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file.shape[0]\n",
    "    \n",
    "\n",
    "    def load_data(self,file_name,label):\n",
    "        #读取数据\n",
    "        mat_file = self.base_file+'/training2017/'+file_name+'.mat'\n",
    "        data = io.loadmat(mat_file)['val']\n",
    "        if label=='N':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='O':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='A':\n",
    "            one_hot=torch.tensor([1])\n",
    "        elif label=='~':\n",
    "            one_hot=torch.tensor([0])\n",
    "            \n",
    "        return data, one_hot\n",
    "\n",
    "\n",
    "    \n",
    "    def crop_padding(self,data,time):\n",
    "        #随机crop\n",
    "        if data.shape[0]<=time:\n",
    "            data=np.pad(data, (0,time-data.shape[0]), 'constant')\n",
    "        elif data.shape[0]>time:\n",
    "            end_index=data.shape[0]-time\n",
    "            start=np.random.randint(0, end_index)\n",
    "            data=data[start:start+time]\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def data_process(self,data):\n",
    "        # 学习论文以及数据集选择合适和采样率\n",
    "        # 并完成随机gaussian 噪声和随机时间尺度放缩\n",
    "        time_scale = np.random.randint(1, 4, 1)\n",
    "        time_scale = int(time_scale[0])\n",
    "        data = data[::time_scale]\n",
    "        # data = data[::3]\n",
    "        data = data-data.mean()\n",
    "        data = data/data.std()\n",
    "        # gassian_noise = np.random.normal(0, 1, data.shape)\n",
    "        # data += gassian_noise\n",
    "        data = self.crop_padding(data, 2400)\n",
    "        data = torch.tensor(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file[idx][1]\n",
    "        label = self.file[idx][2]\n",
    "        data,one_hot = self.load_data(file_name,label)\n",
    "        data = self.data_process(data[0]).unsqueeze(0).float()\n",
    "        one_hot = one_hot.unsqueeze(0).float()\n",
    "\n",
    "        return data, one_hot, file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "def save_loss(fold, value):\n",
    "    path = 'loss' + str(fold) + '.txt'\n",
    "    file = open(path, mode='a+')\n",
    "    file.write(str(value)+'\\n')  \n",
    "    \n",
    "# We will use GPU if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "\n",
    "# Build pre-processing transformation \n",
    "# Note this pre-processing is in PyTorch\n",
    "x_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "])\n",
    "y_transforms = transforms.ToTensor()\n",
    "\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs)\n",
    "\n",
    "# TODO: fine tune hyper-parameters\n",
    "batch_size = 128\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion2 = torch.nn.BCELoss()\n",
    "criterion3 = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer1 = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "train_ecg_dataset = ECG_dataset('./', is_train=True)\n",
    "train_dataloader = DataLoader(train_ecg_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_ecg_dataset = ECG_dataset('./', is_train=False)\n",
    "test_dataloaders = DataLoader(test_ecg_dataset, batch_size=1)\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss:0.00000000: 100%|██████████| 54/54 [00:19<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.4758135444151275\n",
      "Accuracy: 0.08914956011730206\n",
      "Loss: 0.0\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss:0.00000000: 100%|██████████| 54/54 [00:18<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.4655526238639695\n",
      "Accuracy: 0.20234604105571846\n",
      "Loss: 0.0\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss:0.00000000: 100%|██████████| 54/54 [00:19<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.46379360891234245\n",
      "Accuracy: 0.4621700879765396\n",
      "Loss: 0.0\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss:0.00000000: 100%|██████████| 54/54 [00:19<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.4621811785400176\n",
      "Accuracy: 0.48563049853372436\n",
      "Loss: 0.0\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss:0.00000000:  61%|██████    | 33/54 [00:12<00:07,  2.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     49\u001b[0m process \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, name \u001b[38;5;129;01min\u001b[39;00m process:\n\u001b[0;32m     51\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[23], line 88\u001b[0m, in \u001b[0;36mECG_dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile[idx][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     87\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile[idx][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 88\u001b[0m data,one_hot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_process(data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     90\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m one_hot\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mECG_dataset.load_data\u001b[1;34m(self, file_name, label)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m,file_name,label):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m#读取数据\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     mat_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_file\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/training2017/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_file\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     44\u001b[0m         one_hot\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:227\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     mdict\u001b[38;5;241m.\u001b[39mupdate(matfile_dict)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\scipy\\io\\matlab\\_mio4.py:398\u001b[0m, in \u001b[0;36mMatFile4Reader.get_variables\u001b[1;34m(self, variable_names)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_read()\n\u001b[0;32m    397\u001b[0m mdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_of_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    399\u001b[0m     hdr, next_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_var_header()\n\u001b[0;32m    400\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hdr\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hdr\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\scipy\\io\\matlab\\_miobase.py:404\u001b[0m, in \u001b[0;36mMatFileReader.end_of_stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mend_of_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 404\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmat_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     curpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_stream\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_stream\u001b[38;5;241m.\u001b[39mseek(curpos\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, f1_score, recall_score, cohen_kappa_score\n",
    "\n",
    "def validation(model, criterion, test_dataloaders, device):\n",
    "    # TODO: add more metrics for evaluation?\n",
    "    # Evaluate \n",
    "    model.eval()\n",
    "    predict = np.array([])\n",
    "    target = np.array([])\n",
    "    loss=0\n",
    "    step=0\n",
    "    with torch.no_grad():\n",
    "        for x, mask, name in test_dataloaders:\n",
    "            step += 1\n",
    "            mask = mask.to(device)\n",
    "            y = model(x.to(device))\n",
    "            loss +=criterion(y, mask.squeeze(2)).item()\n",
    "            y[y >= 0.5] = 1\n",
    "            y[y < 0.5] = 0\n",
    "            predict=np.append(predict, torch.squeeze(y).cpu().numpy())\n",
    "            target=np.append(target, torch.squeeze(mask).cpu().numpy())\n",
    "    acc = accuracy_score(target, predict)\n",
    "    math_c = matthews_corrcoef(target, predict)\n",
    "    roc_auc_s = roc_auc_score(target, predict)\n",
    "    f1_sco = f1_score(target, predict)\n",
    "    print('Accuracy: {}'.format(acc))\n",
    "    print('Loss:', loss/step)\n",
    "    model.train()\n",
    "\n",
    "    return loss/step, acc, math_c, roc_auc_s, f1_sco\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "matthews_c = []\n",
    "ras = []\n",
    "f1_s = []\n",
    "recall_s = []\n",
    "cohen_ks = []\n",
    "\n",
    "# Start training !\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "        predict = np.array([])\n",
    "        target = np.array([])\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        dt_size = len(train_dataloader.dataset)\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        process = tqdm(train_dataloader)\n",
    "        for x, y, name in process:\n",
    "            step += 1\n",
    "            inputs = x.to(device)\n",
    "            labels = y.to(device)\n",
    "            optimizer1.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion3(outputs, labels.squeeze(2))\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "            epoch_loss += loss.item()\n",
    "            process.set_description(\n",
    "                \"epoch: %d, train_loss:%0.8f\" % (epoch, epoch_loss / step)\n",
    "            )\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            predict=np.append(predict,torch.squeeze(outputs).detach().cpu().numpy())\n",
    "            target=np.append(target,torch.squeeze(labels).detach().cpu().numpy())\n",
    "        epoch_loss /= step\n",
    "        train_loss.append(epoch_loss)\n",
    "        acc = accuracy_score(target, predict)\n",
    "        train_acc.append(acc)\n",
    "        recall_sco = recall_score(target, predict)\n",
    "        cohen_kappa_sco = cohen_kappa_score(target, predict)\n",
    "        print('train_Accuracy: {}'.format(acc))\n",
    "        save_loss(10, epoch_loss)\n",
    "        v_loss, v_acc, math_c, roc_auc_s, f1_src = validation(model, criterion3, test_dataloaders, device)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        matthews_c.append(math_c)\n",
    "        f1_s.append(f1_src)\n",
    "        recall_s.append(recall_sco)\n",
    "        cohen_ks.append(cohen_kappa_sco)\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'weights10_%d.pth' % (epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506009967751392 0.12196802637643284 0.955425219941349 0.1137430252095912\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(max(train_acc), min(train_loss), max(val_acc), min(val_loss))\n",
    "\n",
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "\n",
    "# def save_indicators(indicators_name, indicators, batch_size, x):\n",
    "#     path = 'model_'+str(x)+'_indicator_' + str(batch_size) +'.txt'\n",
    "#     file = open(path, mode='a+')\n",
    "#     for item in indicators_name:\n",
    "#         file.write(str(item)+\", \")\n",
    "    \n",
    "#     file.write('\\n')\n",
    "#     for i in range(len(indicators[0])):\n",
    "#         for j in range(len(indicators)):\n",
    "#             file.write(str(float(indicators[j][i])))\n",
    "#             if j < len(indicators) - 1:\n",
    "#                 file.write(', ')\n",
    "#         file.write('\\n')\n",
    "\n",
    "# indicators = []\n",
    "# indicators_name = ['train loss', 'train accuracy', 'validation loss', 'validation accuracy', 'matthews corrcoef',\n",
    "#             'f1 score', 'recall score', 'cohen kappa score']\n",
    "# indicators.append(train_loss)\n",
    "# indicators.append(train_acc)\n",
    "# indicators.append(val_loss)\n",
    "# indicators.append(val_acc)\n",
    "# indicators.append(matthews_c)\n",
    "# # indicators.append(ras)\n",
    "# indicators.append(f1_s)\n",
    "# indicators.append(recall_s)\n",
    "# indicators.append(cohen_ks)\n",
    "\n",
    "# save_indicators(indicators_name, indicators, batch_size, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79495757624adbb2ec94ee769202e9d295b98634352b3931ea30316c5ea35353"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('pujin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
