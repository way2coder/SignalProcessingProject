{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will apply an CNN to extract features and implement a classification task.\n",
    "# Firstly, we should build the model by PyTorch. We provide a baseline model here.\n",
    "# You can use your own model for better performance\n",
    "class Doubleconv_33(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_33, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_35(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_35, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_37(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_37, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Tripleconv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Tripleconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc(input)\n",
    "\n",
    "\n",
    "class Mscnn(nn.Module):\n",
    "    # TODO: Build a better model\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Mscnn, self).__init__()\n",
    "        self.conv11 = Doubleconv_33(ch_in, 64)\n",
    "        self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv12 = Doubleconv_33(64, 128)\n",
    "        self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv13 = Tripleconv(128, 256)\n",
    "        self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv14 = Tripleconv(256, 512)\n",
    "        self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv15 = Tripleconv(512, 512)\n",
    "        self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.conv21 = Doubleconv_37(ch_in, 64)\n",
    "        self.pool21 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv22 = Doubleconv_37(64, 128)\n",
    "        self.pool22 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv23 = Tripleconv(128, 256)\n",
    "        self.pool23 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv24 = Tripleconv(256, 512)\n",
    "        self.pool24 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv25 = Tripleconv(512, 512)\n",
    "        self.pool25 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.out = MLP(512*27*2, ch_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c11 = self.conv11(x)\n",
    "        p11 = self.pool11(c11)\n",
    "        c12 = self.conv12(p11)\n",
    "        p12 = self.pool12(c12)\n",
    "        c13 = self.conv13(p12)\n",
    "        p13 = self.pool13(c13)\n",
    "        c14 = self.conv14(p13)\n",
    "        p14 = self.pool14(c14)\n",
    "        c15 = self.conv15(p14)\n",
    "        p15 = self.pool15(c15)\n",
    "\n",
    "        c21 = self.conv21(x)\n",
    "        p21 = self.pool21(c21)\n",
    "        c22 = self.conv22(p21)\n",
    "        p22 = self.pool22(c22)\n",
    "        c23 = self.conv23(p22)\n",
    "        p23 = self.pool23(c23)\n",
    "        c24 = self.conv24(p23)\n",
    "        p24 = self.pool24(c24)\n",
    "        c25 = self.conv25(p24)\n",
    "        p25 = self.pool25(c25)\n",
    "\n",
    "        merge = torch.cat((p15, p25), dim=1)\n",
    "        merge = merge.view(merge.size()[0], -1)\n",
    "        output = self.out(merge)\n",
    "        output = F.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to construct the data loader for training. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.io as io\n",
    "import torch\n",
    "\n",
    "# Random clipping has been implemented, \n",
    "# and you need to add noise and random scaling. \n",
    "# Generally, the scaling should be done before the crop.\n",
    "# In general, do not add scaling and noise enhancement options during testing\n",
    "\n",
    "class ECG_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, base_file, cv=0, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self.file_list=[]\n",
    "        self.base_file=base_file\n",
    "        \n",
    "        for i in range(5):\n",
    "            data=pd.read_csv(base_file+'/cv/cv'+str(i)+'.csv')\n",
    "            self.file_list.append(data.to_numpy())\n",
    "        self.file=None\n",
    "        if is_train:\n",
    "            del self.file_list[cv]\n",
    "            self.file=self.file_list[0]\n",
    "            for i in range(1,4):\n",
    "                self.file=np.append(self.file,self.file_list[i],axis=0)\n",
    "        else:\n",
    "            self.file=self.file_list[cv]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file.shape[0]\n",
    "    \n",
    "\n",
    "    def load_data(self,file_name,label):\n",
    "        #读取数据\n",
    "        mat_file = self.base_file+'/training2017/'+file_name+'.mat'\n",
    "        data = io.loadmat(mat_file)['val']\n",
    "        if label=='N':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='O':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='A':\n",
    "            one_hot=torch.tensor([1])\n",
    "        elif label=='~':\n",
    "            one_hot=torch.tensor([0])\n",
    "            \n",
    "        return data, one_hot\n",
    "\n",
    "\n",
    "    \n",
    "    def crop_padding(self,data,time):\n",
    "        #随机crop\n",
    "        if data.shape[0] <= time:\n",
    "            data = np.pad(data, (0,time-data.shape[0]), 'constant')\n",
    "        elif data.shape[0] > time:\n",
    "            end_index = data.shape[0]-time\n",
    "            start = np.random.randint(0, end_index)\n",
    "            data = data[start:start+time]\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def data_process(self,data):\n",
    "        # 学习论文以及数据集选择合适的采样率\n",
    "        # 并完成随机gaussian 噪声和随机时间尺度放缩\n",
    "        time_scale = np.random.randint(1, 4, 1)\n",
    "        time_scale = int(time_scale[0])\n",
    "        data = data[::time_scale]\n",
    "        data = data-data.mean()\n",
    "        data = data/data.std()\n",
    "        gassian_noise = np.random.normal(0, 1, data.shape)\n",
    "        data += gassian_noise\n",
    "        data = self.crop_padding(data, 2400)\n",
    "        data = torch.tensor(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file[idx][1]\n",
    "        label = self.file[idx][2]\n",
    "        data,one_hot = self.load_data(file_name,label)\n",
    "        data = self.data_process(data[0]).unsqueeze(0).float()\n",
    "        one_hot = one_hot.unsqueeze(0).float()\n",
    "\n",
    "        return data, one_hot, file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早停\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch, batch_size):\n",
    "        score = val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch, batch_size)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch, batch_size)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch, batch_size):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'weights10_%d_%d_%f.pth' % (epoch, batch_size, val_loss))\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# We will use GPU if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "\n",
    "# Build pre-processing transformation \n",
    "# Note this pre-processing is in PyTorch\n",
    "x_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "])\n",
    "y_transforms = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# TODO: fine tune hyper-parameters\n",
    "# batch_size = 32\n",
    "# batch_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "# 损失函数\n",
    "criterion1 = torch.nn.MSELoss()\n",
    "criterion2 = torch.nn.BCELoss()\n",
    "criterion3 = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion1\n",
    "num_epochs = 100\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 学习率策略\n",
    "# scheduler1 = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[25, 50, 75], gamma=0.1)\n",
    "# scheduler2 = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs)\n",
    "# scheduler3 = lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "# scheduler = scheduler3\n",
    "\n",
    "#  early stop\n",
    "early_stop = EarlyStopping()\n",
    "\n",
    "# train_ecg_dataset = ECG_dataset('./', is_train=True)\n",
    "# train_dataloader = DataLoader(train_ecg_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# test_ecg_dataset = ECG_dataset('./', is_train=False)\n",
    "# test_dataloaders = DataLoader(test_ecg_dataset, batch_size=1)\n",
    "trainDatasets = [] \n",
    "trainDataloaders = []\n",
    "validDatasets = []\n",
    "validDataloaders = []\n",
    "for i in range(0, 5):\n",
    "    traindataset_i = ECG_dataset('./',cv=i, is_train=True)\n",
    "    traindataloader_i = DataLoader(traindataset_i, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testdataset_i = ECG_dataset('./',cv=i, is_train=False)\n",
    "    testdataloader_i = DataLoader(testdataset_i, batch_size=1)\n",
    "    trainDatasets.append(traindataset_i)\n",
    "    trainDataloaders.append(traindataloader_i)\n",
    "    validDatasets.append(testdataset_i)\n",
    "    validDataloaders.append(testdataloader_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model 0\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss:0.35410175: 100%|██████████| 54/54 [00:28<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.8843447669305189\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.38139619880075465\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss:0.29473089: 100%|██████████| 54/54 [00:26<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.30738797453015754\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss:0.29623118: 100%|██████████| 54/54 [00:25<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9142480211081794\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.30492274899614696\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss:0.29705873: 100%|██████████| 54/54 [00:24<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3025883828560191\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss:0.29542789: 100%|██████████| 54/54 [00:19<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.2992795762870887\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss:0.29528977: 100%|██████████| 54/54 [00:19<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9139548519495749\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.30378747266933015\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss:0.29335257: 100%|██████████| 54/54 [00:20<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3102197537008848\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8, train_loss:0.29272304: 100%|██████████| 54/54 [00:19<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3091235783838058\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, train_loss:0.29558109: 100%|██████████| 54/54 [00:19<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3054390827057299\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10, train_loss:0.29452473: 100%|██████████| 54/54 [00:19<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.30531487230681253\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11, train_loss:0.28691591: 100%|██████████| 54/54 [00:19<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3098276134438354\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12, train_loss:0.28851642: 100%|██████████| 54/54 [00:19<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9139548519495749\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3209321851582646\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13, train_loss:0.28879569: 100%|██████████| 54/54 [00:19<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9138082673702727\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3034463953680176\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14, train_loss:0.28401430: 100%|██████████| 54/54 [00:19<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9129287598944591\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.2977144161954828\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15, train_loss:0.28690574: 100%|██████████| 54/54 [00:19<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9139548519495749\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.29018048343202235\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16, train_loss:0.27865365: 100%|██████████| 54/54 [00:19<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9129287598944591\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.2973708860029923\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17, train_loss:0.27985665: 100%|██████████| 54/54 [00:19<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9143946056874817\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.2900613401401952\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18, train_loss:0.28123283: 100%|██████████| 54/54 [00:19<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9133685136323659\n",
      "Accuracy: 0.8856304985337243\n",
      "Loss: 0.3455656319266715\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19, train_loss:0.27390832: 100%|██████████| 54/54 [00:20<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.2931634724686398\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20, train_loss:0.27235757: 100%|██████████| 54/54 [00:20<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9142480211081794\n",
      "Accuracy: 0.9114369501466275\n",
      "Loss: 0.2912117636120739\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21, train_loss:0.27115438: 100%|██████████| 54/54 [00:19<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9136616827909704\n",
      "Accuracy: 0.6592375366568914\n",
      "Loss: 0.6037707689963827\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22, train_loss:0.27247919: 100%|██████████| 54/54 [00:19<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9129287598944591\n",
      "Accuracy: 0.9008797653958944\n",
      "Loss: 0.39839027892372125\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23, train_loss:0.27017373: 100%|██████████| 54/54 [00:19<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9139548519495749\n",
      "Accuracy: 0.9090909090909091\n",
      "Loss: 0.32402780957498156\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24, train_loss:0.27242726: 100%|██████████| 54/54 [00:20<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9135150982116681\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3065494571416434\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25, train_loss:0.27708665: 100%|██████████| 54/54 [00:20<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9121958369979478\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.31345211621210023\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26, train_loss:0.26953123: 100%|██████████| 54/54 [00:20<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9135150982116681\n",
      "Accuracy: 0.9102639296187683\n",
      "Loss: 0.26814126666591576\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27, train_loss:0.26253133: 100%|██████████| 54/54 [00:20<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9126355907358545\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.3103182661371672\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28, train_loss:0.26398182: 100%|██████████| 54/54 [00:20<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.9120234604105572\n",
      "Loss: 0.30039829268636414\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29, train_loss:0.26121149: 100%|██████████| 54/54 [00:20<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9143946056874817\n",
      "Accuracy: 0.9114369501466275\n",
      "Loss: 0.3281599831240268\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30, train_loss:0.26214802: 100%|██████████| 54/54 [00:20<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9133685136323659\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.29154612359965\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 31, train_loss:0.25867318: 100%|██████████| 54/54 [00:21<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9139548519495749\n",
      "Accuracy: 0.910850439882698\n",
      "Loss: 0.32170757356036966\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 32, train_loss:0.26490257: 100%|██████████| 54/54 [00:20<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9132219290530637\n",
      "Accuracy: 0.9120234604105572\n",
      "Loss: 0.26992639760962\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 33, train_loss:0.25779259: 100%|██████████| 54/54 [00:19<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9127821753151568\n",
      "Accuracy: 0.9131964809384164\n",
      "Loss: 0.26662669882855633\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 34, train_loss:0.25781277: 100%|██████████| 54/54 [00:19<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9141014365288772\n",
      "Accuracy: 0.9102639296187683\n",
      "Loss: 0.2558968711087256\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 35, train_loss:0.24962230: 100%|██████████| 54/54 [00:19<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.9143946056874817\n",
      "Accuracy: 0.9120234604105572\n",
      "Loss: 0.3139060786960761\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     86\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 87\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion2\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LNet\\lib\\site-packages\\torch\\nn\\functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3096\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, f1_score, recall_score, cohen_kappa_score\n",
    "\n",
    "def save_indicators(indicators_name, indicators, batch_size, i):\n",
    "    path = 'model_'+str(i)+'_indicator_' + str(batch_size) +'.txt'\n",
    "    file = open(path, mode='a+')\n",
    "    for item in indicators_name:\n",
    "        file.write(str(item)+\", \")\n",
    "    \n",
    "    file.write('\\n')\n",
    "    for i in range(len(indicators[0])):\n",
    "        for j in range(len(indicators)):\n",
    "            file.write(str(float(indicators[j][i])))\n",
    "            if j < len(indicators) - 1:\n",
    "                file.write(', ')\n",
    "        file.write('\\n')\n",
    "\n",
    "def validation(model, criterion, test_dataloaders, device):\n",
    "    # TODO: add more metrics for evaluation?\n",
    "    # Evaluate \n",
    "    model.eval()\n",
    "    predict = np.array([])\n",
    "    target = np.array([])\n",
    "    loss=0\n",
    "    step=0\n",
    "    with torch.no_grad():\n",
    "        for x, mask, name in test_dataloaders:\n",
    "            step += 1\n",
    "            mask = mask.to(device)\n",
    "            y = model(x.to(device))\n",
    "            loss += criterion(y, mask.squeeze(2)).item()\n",
    "            y[y >= 0.5] = 1\n",
    "            y[y < 0.5] = 0\n",
    "            predict=np.append(predict, torch.squeeze(y).cpu().numpy())\n",
    "            target=np.append(target, torch.squeeze(mask).cpu().numpy())\n",
    "    acc = accuracy_score(target, predict)\n",
    "    math_c = matthews_corrcoef(target, predict)\n",
    "    roc_auc_s = roc_auc_score(target, predict)\n",
    "    f1_sco = f1_score(target, predict)\n",
    "    recall_sco = recall_score(target, predict)\n",
    "    cohen_kappa_sco = cohen_kappa_score(target, predict)\n",
    "    print('Accuracy: {}'.format(acc))\n",
    "    print('Loss:', loss/step)\n",
    "    model.train()\n",
    "\n",
    "    return loss/step, acc, math_c, roc_auc_s, f1_sco, recall_sco, cohen_kappa_sco\n",
    "\n",
    "for i in range(0, 1):\n",
    "    model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # scheduler1 = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[25, 50, 75], gamma=0.1)\n",
    "    # scheduler2 = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs)\n",
    "    # scheduler3 = lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "    # scheduler = scheduler2\n",
    "    train_dataloader = trainDataloaders[i]\n",
    "    test_dataloaders = validDataloaders[i]\n",
    "\n",
    "# 指标list\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    matthews_c = []\n",
    "    ras = []\n",
    "    f1_s = []\n",
    "    recall_s = []\n",
    "    cohen_ks = []\n",
    "    indicators = []\n",
    "    indicators_name = ['train loss', 'train accuracy', 'validation loss', 'validation accuracy', 'matthews corrcoef',\n",
    "            'roc auc score', 'f1 score', 'recall score', 'cohen kappa score']\n",
    "\n",
    "    # Start training !\n",
    "    print(f\"start training model {i}\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "            predict = np.array([])\n",
    "            target = np.array([])\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "            dt_size = len(train_dataloader.dataset)\n",
    "            epoch_loss = 0\n",
    "            step = 0\n",
    "            process = tqdm(train_dataloader)\n",
    "            for x, y, name in process:\n",
    "                step += 1\n",
    "                inputs = x.to(device)\n",
    "                labels = y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion2(outputs, labels.squeeze(2))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                process.set_description(\n",
    "                    \"epoch: %d, train_loss:%0.8f\" % (epoch, epoch_loss / step)\n",
    "                )\n",
    "                # print(outputs)\n",
    "                outputs[outputs >= 0.5] = 1\n",
    "                outputs[outputs < 0.5] = 0\n",
    "                predict = np.append(predict, torch.squeeze(outputs).detach().cpu().numpy())\n",
    "                target = np.append(target, torch.squeeze(labels).detach().cpu().numpy())\n",
    "            # scheduler.step()\n",
    "            epoch_loss /= step\n",
    "            acc = accuracy_score(target, predict)\n",
    "            print('train_Accuracy: {}'.format(acc))\n",
    "            v_loss, v_acc, math_c, roc_auc_s, f1_src, recall_sco, cohen_kappa_sco = validation(model, criterion2, test_dataloaders, device)\n",
    "            train_loss.append(epoch_loss)\n",
    "            train_acc.append(acc)\n",
    "            val_loss.append(v_loss)\n",
    "            val_acc.append(v_acc)\n",
    "            matthews_c.append(math_c)\n",
    "            f1_s.append(f1_src)\n",
    "            recall_s.append(recall_sco)\n",
    "            ras.append(roc_auc_s)\n",
    "            cohen_ks.append(cohen_kappa_sco)\n",
    "            # early_stop(v_loss, model, epoch, batch_size)\n",
    "    # Save model\n",
    "            \n",
    "    torch.save(model.state_dict(), 'model_%d_weights10_%d_%d_%f.pth' % (i, epoch, batch_size, v_loss))\n",
    "    indicators.append(train_loss)\n",
    "    indicators.append(train_acc)\n",
    "    indicators.append(val_loss)\n",
    "    indicators.append(val_acc)\n",
    "    indicators.append(matthews_c)\n",
    "    indicators.append(ras)\n",
    "    indicators.append(f1_s)\n",
    "    indicators.append(recall_s)\n",
    "    indicators.append(cohen_ks)\n",
    "\n",
    "    save_indicators(indicators_name, indicators, batch_size, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "\n",
    "# def save_indicators(indicators_name, indicators, batch_size, scheduler):\n",
    "#     path = 'indicator_' + str(batch_size) + '.txt'\n",
    "#     file = open(path, mode='a+')\n",
    "#     for item in indicators_name:\n",
    "#         file.write(str(item)+\", \")\n",
    "    \n",
    "#     file.write('\\n')\n",
    "#     for i in range(len(indicators[0])):\n",
    "#         for j in range(len(indicators)):\n",
    "#             file.write(str(float(indicators[j][i])))\n",
    "#             if j < len(indicators) - 1:\n",
    "#                 file.write(', ')\n",
    "#         file.write('\\n')\n",
    "\n",
    "# indicators.append(train_loss)\n",
    "# indicators.append(train_acc)\n",
    "# indicators.append(val_loss)\n",
    "# indicators.append(val_acc)\n",
    "# indicators.append(matthews_c)\n",
    "# indicators.append(ras)\n",
    "# indicators.append(f1_s)\n",
    "# indicators.append(recall_s)\n",
    "# indicators.append(cohen_ks)\n",
    "\n",
    "# save_indicators(indicators_name, indicators, batch_size, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79495757624adbb2ec94ee769202e9d295b98634352b3931ea30316c5ea35353"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('pujin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
